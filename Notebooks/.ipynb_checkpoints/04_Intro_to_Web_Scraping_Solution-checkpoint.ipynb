{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Web Scraping with Beautiful Soup\n",
    "\n",
    "This tutorial will provide a very basic introduction to using the Beautiful Soup package to scrape text data from the web. \n",
    "\n",
    "##Installation\n",
    "\n",
    "In a terminal, install Beautiful Soup if necessary by running <pre><code>conda install beautiful-soup</code></pre>\n",
    "\n",
    "##Retrieving HTML\n",
    "\n",
    "The general idea behind web scraping is to retrieve data that exists on a website, and convert it into a format that is usable for analysis. Webpages are rendered by the brower from HTML and CSS code, but much of the information included in the HTML underlying any website is not interesting to us.\n",
    "\n",
    "We begin by reading in the source code for a given web page and creating a Beautiful Soup object with the BeautifulSoup function.\n",
    "\n",
    "###urllib2\n",
    "urllib2 is a module for working with urls, we will use it to open connections to urls and retrieve the webpage source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "page = urllib2.urlopen('http://espn.go.com/nba/statistics/player/_/stat/scoring-per-game/sort/avgPoints/qualified/false').read()\n",
    "soup = BeautifulSoup(page)\n",
    "print type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soup object contains all of the HTML in the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.prettify()[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Parsing HTML\n",
    "\n",
    "By \"parsing HTML\", we mean pulling out only the relevant tags/attributes for our analysis.  What Beautiful Soup does is provide a handy bunch of methods for doing this efficiently.\n",
    "\n",
    "###find method\n",
    "\n",
    "The find method will search for and return the first tag matching your corresponding search criteria, if it exists.  You can specify tag and attribute info etc.  There is also a findAll method that will return a collection of tags matching your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_div = soup.find(id='my-players-table')\n",
    "print table_div.prettify()[0:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the object returned by the find method is just another inner HTML structure, which we can step through just like we would have with the original soup object.  We've gone to the location in the webpage where the table that we seek starts, now we can use find again to get to the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = table_div.find(\"table\")\n",
    "print table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Searching by Attributes\n",
    "\n",
    "Now that we have the table object, we need to step through the rows.  First we'll find the header row so we can populate what the field names will be in our data.  Here we're searching for tags under the table tag whose class attritbute is \"colhead\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_head = table.find(attrs={\"class\":'colhead'})\n",
    "print table_head.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find the actual values by searching for the 'td' tags, which is the tag for table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header_cols = table_head.findAll('td')\n",
    "print header_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we step through these columns and save them to a list to be used later.  We'll ignore the rank column (RK) because that doesn't give us anything we want later.  We also separate the **PLAYER** column into **PLAYER** and **POSITION**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = []\n",
    "for header_col in header_cols:\n",
    "    val = header_col.string\n",
    "    if val != 'RK':\n",
    "        cols.append(val)\n",
    "    if val == 'PLAYER':\n",
    "        cols.append('POSITION')\n",
    "print cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stepping Through a Table\n",
    "\n",
    "The table rows are indicated by the tag 'tr'.  Again we can find them all and iterate through them.  Within each row we iterate through the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_rows = table.findAll('tr')\n",
    "print table_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save our results in 2 different ways to demonstrate how we can handle both.  The first will be a list of dicts where the key is the field name and the value is the field value.  The second will just be a list of lists of stats with no field name values (we've already defined them earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players_stats_dicts = []\n",
    "players_stats_array = []\n",
    "for row in table_rows:\n",
    "    if row.attrs['class'][0]=='colhead':\n",
    "        continue\n",
    "    player_stats = []\n",
    "    row_cols = row.find_all('td')\n",
    "    col_vals = []\n",
    "    player_col = row_cols[1]\n",
    "    player_name = player_col.find('a').string\n",
    "    player_position = player_col.contents[1]\n",
    "    player_position = player_position.split(' ')[1]\n",
    "    player_stats.append(player_name)\n",
    "    player_stats.append(player_position)\n",
    "    for i in range(2, len(row_cols)):\n",
    "        stat = row_cols[i].string\n",
    "        player_stats.append(stat)\n",
    "    players_stats_array.append(player_stats)\n",
    "    player_stats = dict(zip(cols, player_stats))\n",
    "    players_stats_dicts.append(player_stats)\n",
    "print players_stats_dicts[0:5]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used the zip function to combine pairs of lists into tuples, and then transformed that into a dict to get a dictionary of FIELD --> VALUE for every player in the table.\n",
    "\n",
    "Beautiful Soup has many other features, including the ability to step up, down, and sideways in the HTML tree and basically search for any tags, attributes, or values.  For more, take a look at the [Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Load into Pandas\n",
    "\n",
    "Let's load our scraped data into Pandas and take a look at it.  Here is the first way we can do it, simply directly from the dictionary we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame.from_dict(players_stats_dicts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is a 2nd way we can do it.  We convert the 2D stats array into a numpy array and create a Pandas dataframe from it along with the list of column headers we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_array = np.array(players_stats_array)\n",
    "df = pd.DataFrame(np_array, columns=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exercise\n",
    "\n",
    "The goal of this exercise is to combine the scoring and assists statistics for every player in the NBA in 2014-2015.  The end result will have them in a pandas dataframe with the fields from both pages for every player.\n",
    "\n",
    "The general steps should be as follows:\n",
    "- Create a function get_cols that retrieves the names of the header columns given a table element (skip the ranks, split the positions)\n",
    "- Create a function get_data that retrieves the actual table data given a table element (skip the ranks, split the positions).  You can use either the dict approach or the numpy array approach.\n",
    "- Write a python loop to loop through the various pages and call these functions on the appropriate urls so that you can retrieve every player (rather than just the top few).\n",
    "- Repeat the above on both the scoring and assists URLs to get a pandas dataframe for both of them\n",
    "- Use the pandas.DataFrame.join() function to join your 2 pandas dataframes together and get a total result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
