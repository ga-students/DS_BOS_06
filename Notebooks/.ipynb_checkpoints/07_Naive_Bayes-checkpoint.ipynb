{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes Classifiers\n",
    "[Naive Bayes Classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) are a thoroughly studied and popular option for classification because of their ease of training, scalability (linear in the number of features) in training and prediction, and intuitive probabilistic understanding.  Despite their rather stringent (\"naive\") restrictions in the **assumption that the features are conditionally independent**, they are still often able to perform reasonably well in the right situations.  Particularly, Naive Bayes Classifiers have historically been frequently used as (baseline) methods for classification of raw text documents, so those are the type of tasks that we will investigate with them today.\n",
    "\n",
    "##Learning Goals\n",
    "- Understand how the [Multinomial Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes) model is used for [**text classification**](https://en.wikipedia.org/wiki/Document_classification)\n",
    "- Introduction to the [Bag of Words Model](https://en.wikipedia.org/wiki/Bag-of-words_model) for text analysis\n",
    "- Create features out of raw text data with `sklearn`\n",
    "- Evaluate various classification performance metrics with `sklearn`\n",
    "\n",
    "##Datasets\n",
    "- [Amazon Review Sentiment](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences): single sentences from raw-text Amazon reviews where each review has been labeled as having positive, negative, or neutral sentiment\n",
    "- [SMS Spam Classification](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection): set of spam and non-spam text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Amazon Reviews Sentiment Analysis\n",
    "We'll analyze this first dataset together with Naive Bayes.  We have a collection of reviews from Amazon where each is labeled as having either positive or negative sentiment (some are neutral).  Our goal will be to use `sklearn` to build a Naive Bayes Classifier that is able to accurately report (predict) whether a new review is positive or negative in its sentiment.\n",
    "\n",
    "###Get the Data\n",
    "Let's retrieve our usual imports and load the data into a `pandas.DataFrame` to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data in\n",
    "df = pd.read_csv(\"/Users/pburkard88/Downloads/sentiment labelled sentences/amazon_cells_labelled.txt\", sep='\\t', names=['text', 'sentiment'])\n",
    "# Take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Remove Neutral (Null) Reviews\n",
    "Notice that many of the sentiment values are reported as ***NaN***.  We'll want to remove these and work with only the labeled observations when creating our classifier.  Let's take a look at just how many of our entries are ***NaN*** by calling `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that there are 14609 total reviews but only 1000 of them actually have sentiment labels.  We need a way to extract only these observations.  To do this we'll use the `pandas` [**`dropna()`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the NaN rows\n",
    "df = df.dropna()\n",
    "# Check out what our df looks like now\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Generate Train/Test Data\n",
    "Now that we have the 1000 labeled reviews, let's split into a 70/30 train/test set with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Split the data into a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extracting Features from Text Data\n",
    "The only feature we currently have for each observation is the raw text of the review.  We need to find a way to transform this into a ***useful vector of numeric features*** for every review.  To do this we'll use what's called a [Bag of Words Model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "####Bag of Words Model\n",
    "An important discovery in text analysis was the realization that a shocking amount of value could be gleaned simply from the words (or ***tokens***) that appear in a text document (irrespective of other considerations like word order, grammar, etc).  This hypothesis is generally referred to the ***Bag of Words*** hypothesis, that just the set of words that make up a document dictate most of the conceptual content of the document.  \n",
    "\n",
    "This suggests some ways that we can turn our raw text into numeric features.  The basic idea is that for a set of text documents to analyze (a ***corpus*** of documents) we could build up a ***dictionary*** of all the words/tokens that ever appear in any of the documents.  Each token in this dictionary then becomes a new feature in our feature space, and thus the number of features for each document will be the total number of tokens in that dictionary.  Now that we have defined a feature space, we can come up with a number of increasingly complex (yet still simple) schemes to provide values for those features...\n",
    "\n",
    "#####Boolean Word Occurrence Model\n",
    "The first, and simplest feature model is one of binary word occurrence.  Simply put, for a given document and for every word in the features dictionary, the feature value for the respective word is set to 1 if the document contains the word and 0 otherwise.  In `sklearn` we can do this simply with the `CountVectorizer` class with the appropriate parameters as shown below.\n",
    "\n",
    "#####Word Frequency Model\n",
    "Slightly more complex than the binary model is a frequency model.  Instead of getting a 1, the document will get the count of occurrences of the word in that document for every word in the document (and 0 otherwise).  Obviously this is encoding more information and thus should be better as long as we can fit it into our framework for classification (and we can).  We will see how easily we can do this below with the `CountVectorizer` class of `sklearn`.\n",
    "\n",
    "#####Weighted Frequency Schemes - TFIDF\n",
    "A step further than the basic frequency model involves adding weights to our frequency feature model that allow us to better capture the way words are distributed in text.  A common example is [TFIDF Weighting](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) which basically is any technique that gives an **upscore** to words occurring many times **within the document (Term Frequency TF)** and a **downscore** to words occurring many times **throughout all documents (Inverse Document Frequency IDF)**.  For instance, glue words like \"the\" and \"and\" have almost zero conceptual value to add and occur many times in all documents, so they will be weighted less.  `sklearn` allows us to easily perform TFIDF weighting with the `TfidfVectorizer` as we will also see below.\n",
    "\n",
    "####Feature Creation with sklearn\n",
    "Before we try out the 3 schemes above, let's quickly understand how [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) works.  It uses a training set to train a Vectorizer object that will then be able to take any future raw text and convert it into an appropriate feature vector.  Here's a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# start with a simple example\n",
    "train_simple = ['call you tonight',\n",
    "                'Call me a cab',\n",
    "                'please call me... PLEASE!']\n",
    "\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer(decode_error = 'ignore')\n",
    "vect.fit(train_simple)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform training data into a 'document-term matrix' (more on this later)\n",
    "train_simple_dtm = vect.transform(train_simple)\n",
    "train_simple_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "test_simple = [\"please don't call me\"]\n",
    "test_simple_dtm = vect.transform(test_simple)\n",
    "test_simple_dtm.toarray()\n",
    "pd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how `CountVectorizer` works, let's try building features for our Amazon Reviews training set.  Let's start with the simple boolean model, here's how we can extract such features immediately with `sklearn` using `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Create a CountVectorizer with binary=True so all nonzero counts are given value 1\n",
    "binary_vect = CountVectorizer(decode_error = 'ignore', binary=True)\n",
    "# Call fit to do our vectorization\n",
    "binary_vect.fit(X_train)\n",
    "# Print out all of the tokens in the dictionary\n",
    "binary_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `CountVectorizer` has created our dictionary for us, with a feature for every token in the training dataset.  **How many tokens do we have?**\n",
    "\n",
    "Now let's upgrade to the frequency model by leaving the `binary` option `False` (the default value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a frequency vectorizer\n",
    "freq_vect = CountVectorizer(decode_error = 'ignore')\n",
    "# Call fit to do our frequency vectorization\n",
    "freq_vect.fit(X_train)\n",
    "# Check out the dictionary of features\n",
    "freq_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before we put our new feature spaces to use, let's try adding some **TFIDF weights** using the [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) of `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a TFIDF vectorizer\n",
    "tfidf_vect = TfidfVectorizer(decode_error = 'ignore')\n",
    "# Call fit to do our frequency vectorization\n",
    "tfidf_vect.fit(X_train)\n",
    "# Check out the dictionary of features\n",
    "tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Document Term Matrix\n",
    "Now that we have our feature vectorizers for the training set, we can use them to generate explicit data matrices to be used in an `sklearn` classifier (our ***X*** matrix).  For this type of problem, this amounts to a matrix with the documents as the rows and the words of the dictionary as the columns, hence the term **Document Term Matrix**.  Generally, this is what will be created as the input for machine learning algorithms operating on a corpus of raw text data.\n",
    "\n",
    "To create this matrix for each of our approaches, we can use the `sklearn` [`transform()`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform) function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the binary model, training and test X matrices\n",
    "binary_train_dtm = binary_vect.transform(X_train)\n",
    "binary_test_dtm = binary_vect.transform(X_test)\n",
    "# For the frequency model, training and test X matrices\n",
    "freq_train_dtm = freq_vect.transform(X_train)\n",
    "freq_test_dtm = freq_vect.transform(X_test)\n",
    "# For the tfidf model, training and test X matrices\n",
    "tfidf_train_dtm = tfidf_vect.transform(X_train)\n",
    "tfidf_test_dtm = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fit and transform operations can be performed in a single step by calling `fit_transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Performing Naive Bayes Classification\n",
    "`sklearn` has several different types of Naive Bayes Classifiers.\n",
    "\n",
    "####Binary Model\n",
    "For the binary model, the features are all assumed to be binary 0/1, which means the [`BernoulliNB`](http://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes) model of `sklearn` is appropriate.  Here's how we would train such a model and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Create the model\n",
    "bnb = BernoulliNB()\n",
    "# Fit the model to the training data\n",
    "bnb.fit(binary_train_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "bnb.score(binary_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Frequency Model\n",
    "For the frequency model, the [`MultinomialNB`](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) model is appropriate because it's assumed that each of the possible features is selected *k* times from a possible *n* (where *n* is the number of words in the document) and thus can be modeled by the [Multinomial Distribution](https://en.wikipedia.org/wiki/Multinomial_distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Create the model\n",
    "mnb = MultinomialNB()\n",
    "# Fit the model to the training data\n",
    "mnb.fit(freq_train_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "mnb.score(freq_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####TFIDF Model\n",
    "Finally, let's try the same with our **TFIDF** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "tfidf_nb = MultinomialNB()\n",
    "# Fit the model to our training data\n",
    "tfidf_nb.fit(tfidf_train_dtm, y_train)\n",
    "# Score the model against our test data\n",
    "tfidf_nb.score(tfidf_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Classifier Model Evaluation\n",
    "As we've touched on, there a number of ways that one can evaluate the performance of a potential classification model.  Let's take a look at a few of them and how we can use them via the [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) package.  We'll use our basic frequency model for all of these.\n",
    "\n",
    "####Classification Accuracy\n",
    "The default `score()` method for a classifier returns the % correct accuracy of the classifier against a given test set.  It can be duplicated by calling the `metrics.accuracy_score()` function against a set of true labels and predicted labels.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from sklearn import metrics\n",
    "# Make class predictions for all observations in the test set\n",
    "y_pred = mnb.predict(freq_test_dtm)\n",
    "# Print classification accuracy\n",
    "metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Confusion Matrix\n",
    "The [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) returns a table that displays the 4 potential options, **true positive, true negative, false positive, false negative**, with counts for each in our test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Precision, Recall, and F1\n",
    "We can also easily display the [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) and [F1](https://en.wikipedia.org/wiki/F1_score) scores for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve precision\n",
    "p = metrics.precision_score(y_test, y_pred)\n",
    "# Retrieve Recall\n",
    "r = metrics.recall_score(y_test, y_pred)\n",
    "# Print precision and recall\n",
    "print 'Precision: ' + str(p)\n",
    "print 'Recall: ' + str(r)\n",
    "# Retrieve F1 from sklearn and print\n",
    "print 'F1: ' + str(metrics.f1_score(y_test, y_pred))\n",
    "# Calculate F1 manually and confirm they're the same\n",
    "print 'Manual F1: ' + str(2*p*r/(p + r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####ROC Curve\n",
    "As we've seen, we can generally increase our recall for a given class at the expense of precision and vice versa.  [ROC Curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) attempt to capture this relationship by ploting the **True Positive Rate** against the **False Positive Rate** across the spectrum of thresholds for predicting a positive class.  The extreme cases are predicting all classes to be positive (large false positive rate) or predicting none or very few to be positive (large true positive rate).  An ideal ROC curve would be a perfect right angle that maintains a 100% True Positive Rate even at 0 False Positive Rate, thus the closer a curve is to this ideal the better our model.  We can measure how close our ROC Curve is to ideal by measuring the area under the curve.\n",
    "\n",
    "We can easily retrieve ROC Curves with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve a series of False Positive Rate and True Positive Rate (with their respective thresholds) for plotting an ROC Curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, mnb.predict_proba(freq_test_dtm)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot our ROC Curve\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(fpr, tpr, label='ROC Curve', color='red')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exercise - Spam Classification\n",
    "\n",
    "We'll step through a similar exercise with the spam classification dataset [here](https://github.com/pburkard88/DS_BOS_06/Data/Spam Classification/sms.csv) (note that this is a pre-parsed version of the one from the UCI website, so make sure to use this one).  The goal is to be able to classify raw text SMS messages as either spam or not spam based on the training set given.  We'll again use a Naive Bayes Multinomial model to do so.\n",
    "\n",
    "First import `numpy` and `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing numpy and pandas \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## READING IN THE DATA\n",
    "\n",
    "# Check it out with head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records are they of each type (spam/not spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out value_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want our *label* column to be binary, so use map to convert that column to 0 for 'ham' and 1 for 'spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the label into a binary variable with map\n",
    "\n",
    "# Use head() to make sure it worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a **70/30 train/test split** with [`sklearn.cross_validation.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) and put the results in `X_train`, `X_test`, `y_train`, and `y_test`.  Give the call to your function the parameter `random_state=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets by calling sklearn lib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some of `X_train` as well as the shape of `X_train` and `X_test` using `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the shape of X_train\n",
    "\n",
    "# Print X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the shape of X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Generating Features\n",
    "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer ( use variable name as vect)\n",
    "\n",
    "# Fit the vectorizer to the training set X_train\n",
    "\n",
    "# Print out the dictionary of terms with get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a vectorizer trained on `X_train`, call `transform()` on both `X_train` and `X_test` to get the **Document Term Matrix** for each.  Store your results into `train_dtm` and `test_dtm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate train_dtm\n",
    "\n",
    "# Generate test_dtm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 50 and last 50 features of your dataset by investigating `vect.get_feature_names()`.  ***HINT:*** It's just a `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First 50\n",
    "\n",
    "# Last 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Build the Model\n",
    "Use `sklearn` to build a `MultinomialNB` classifier against your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import MultinomialNB\n",
    "\n",
    "# Create model\n",
    "\n",
    "# Fit model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluate the Model\n",
    "Use `sklearn.metrics` to retrieve the following evaluation metrics for your model against the test set:\n",
    "- Percent Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- Confusion Matrix\n",
    "- ROC: Plot the curve and return the area under the curve\n",
    "- A classification report via a simple call to `classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test_dtm, call them y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percent Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ROC Curve Area\n",
    "\n",
    "# ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classification Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extra: Playing with pandas\n",
    "Use the arrays train_dtm and test_dtm along with y_train and y_test to see if you can put the data back into dataframes called `df_train` and `df_test`, where the columns are the feature names.\n",
    "- Get the feature names with `get_feature_names()`\n",
    "- Pass the `numpy` array data explicitly to the `pandas.DataFrame` constructor along with the column names from `get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out which words are most indicative of spam vs. not spam, which words appear the most in the dataset, and which words the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
