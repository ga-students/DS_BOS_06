{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Amazon Reviews Sentiment Analysis\n",
    "We'll analyze this first dataset together with Naive Bayes.  We have a collection of reviews from Amazon where each is labeled as having either positive or negative sentiment (some are neutral).  Our goal will be to use `sklearn` to build a Naive Bayes Classifier that is able to accurately report (predict) whether a new review is positive or negative in its sentiment.\n",
    "\n",
    "###Get the Data\n",
    "Let's retrieve our usual imports and load the data into a `pandas.DataFrame` to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I try not to adjust the volume setting to avoi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought Motorola made reliable products!.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Battery for Motorola Razr.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  I try not to adjust the volume setting to avoi...        NaN\n",
       "1  So there is no way for me to plug it in here i...          0\n",
       "2                        Good case, Excellent value.          1\n",
       "3        I thought Motorola made reliable products!.        NaN\n",
       "4                         Battery for Motorola Razr.        NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data in\n",
    "df = pd.read_csv(\"/Users/pburkard88/Downloads/sentiment labelled sentences/amazon_cells_labelled.txt\", sep='\\t', names=['text', 'sentiment'])\n",
    "# Take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Remove Neutral (Null) Reviews\n",
    "Notice that many of the sentiment values are reported as ***NaN***.  We'll want to remove these and work with only the labeled observations when creating our classifier.  Let's take a look at just how many of our entries are ***NaN*** by calling `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14609 entries, 0 to 14608\n",
      "Data columns (total 2 columns):\n",
      "text         14609 non-null object\n",
      "sentiment    1000 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 342.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that there are 14609 total reviews but only 1000 of them actually have sentiment labels.  We need a way to extract only these observations.  To do this we'll use the `pandas` [**`dropna()`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 1 to 2913\n",
      "Data columns (total 2 columns):\n",
      "text         1000 non-null object\n",
      "sentiment    1000 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remove the NaN rows\n",
    "df = df.dropna()\n",
    "# Check out what our df looks like now\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Generate Train/Test Data\n",
    "Now that we have the 1000 labeled reviews, let's split into a 70/30 train/test set with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Split the data into a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extracting Features from Text Data\n",
    "The only feature we currently have for each observation is the raw text of the review.  We need to find a way to transform this into a ***useful vector of numeric features*** for every review.  To do this we'll use what's called a [Bag of Words Model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "####Bag of Words Model\n",
    "An important discovery in text analysis was the realization that a shocking amount of value could be gleaned simply from the words (or ***tokens***) that appear in a text document (irrespective of other considerations like word order, grammar, etc).  This hypothesis is generally referred to the ***Bag of Words*** hypothesis, that just the set of words that make up a document dictate most of the conceptual content of the document.  \n",
    "\n",
    "This suggests some ways that we can turn our raw text into numeric features.  The basic idea is that for a set of text documents to analyze (a ***corpus*** of documents) we could build up a ***dictionary*** of all the words/tokens that ever appear in any of the documents.  Each token in this dictionary then becomes a new feature in our feature space, and thus the number of features for each document will be the total number of tokens in that dictionary.  Now that we have defined a feature space, we can come up with a number of increasingly complex (yet still simple) schemes to provide values for those features...\n",
    "\n",
    "#####Boolean Word Occurrence Model\n",
    "The first, and simplest feature model is one of binary word occurrence.  Simply put, for a given document and for every word in the features dictionary, the feature value for the respective word is set to 1 if the document contains the word and 0 otherwise.  In `sklearn` we can do this simply with the `CountVectorizer` class with the appropriate parameters as shown below.\n",
    "\n",
    "#####Word Frequency Model\n",
    "Slightly more complex than the binary model is a frequency model.  Instead of getting a 1, the document will get the count of occurrences of the word in that document for every word in the document (and 0 otherwise).  Obviously this is encoding more information and thus should be better as long as we can fit it into our framework for classification (and we can).  We will see how easily we can do this below with the `CountVectorizer` class of `sklearn`.\n",
    "\n",
    "#####Weighted Frequency Schemes - TFIDF\n",
    "A step further than the basic frequency model involves adding weights to our frequency feature model that allow us to better capture the way words are distributed in text.  A common example is [TFIDF Weighting](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) which basically is any technique that gives an **upscore** to words occurring many times **within the document (Term Frequency TF)** and a **downscore** to words occurring many times **throughout all documents (Inverse Document Frequency IDF)**.  For instance, glue words like \"the\" and \"and\" have almost zero conceptual value to add and occur many times in all documents, so they will be weighted less.  `sklearn` allows us to easily perform TFIDF weighting with the `TfidfVectorizer` as we will also see below.\n",
    "\n",
    "####Feature Creation with sklearn\n",
    "Before we try out the 3 schemes above, let's quickly understand how [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) works.  It uses a training set to train a Vectorizer object that will then be able to take any future raw text and convert it into an appropriate feature vector.  Here's a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cab', u'call', u'me', u'please', u'tonight', u'you']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# start with a simple example\n",
    "train_simple = ['call you tonight',\n",
    "                'Call me a cab',\n",
    "                'please call me... PLEASE!']\n",
    "\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer(decode_error = 'ignore')\n",
    "vect.fit(train_simple)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix' (more on this later)\n",
    "train_simple_dtm = vect.transform(train_simple)\n",
    "train_simple_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "test_simple = [\"please don't call me\"]\n",
    "test_simple_dtm = vect.transform(test_simple)\n",
    "test_simple_dtm.toarray()\n",
    "pd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how `CountVectorizer` works, let's try building features for our Amazon Reviews training set.  Let's start with the simple boolean model, here's how we can extract such features immediately with `sklearn` using `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Create a CountVectorizer with binary=True so all nonzero counts are given value 1\n",
    "binary_vect = CountVectorizer(decode_error = 'ignore', binary=True)\n",
    "# Call fit to do our vectorization\n",
    "binary_vect.fit(X_train)\n",
    "# Print out all of the tokens in the dictionary\n",
    "binary_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `CountVectorizer` has created our dictionary for us, with a feature for every token in the training dataset.  **How many tokens do we have?**\n",
    "\n",
    "Now let's upgrade to the frequency model by leaving the `binary` option `False` (the default value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a frequency vectorizer\n",
    "freq_vect = CountVectorizer(decode_error = 'ignore')\n",
    "# Call fit to do our frequency vectorization\n",
    "freq_vect.fit(X_train)\n",
    "# Check out the dictionary of features\n",
    "freq_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before we put our new feature spaces to use, let's try adding some **TFIDF weights** using the [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) of `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a TFIDF vectorizer\n",
    "tfidf_vect = TfidfVectorizer(decode_error = 'ignore')\n",
    "# Call fit to do our frequency vectorization\n",
    "tfidf_vect.fit(X_train)\n",
    "# Check out the dictionary of features\n",
    "tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Document Term Matrix\n",
    "Now that we have our feature vectorizers for the training set, we can use them to generate explicit data matrices to be used in an `sklearn` classifier (our ***X*** matrix).  For this type of problem, this amounts to a matrix with the documents as the rows and the words of the dictionary as the columns, hence the term **Document Term Matrix**.  Generally, this is what will be created as the input for machine learning algorithms operating on a corpus of raw text data.\n",
    "\n",
    "To create this matrix for each of our approaches, we can use the `sklearn` [`transform()`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform) function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the binary model, training and test X matrices\n",
    "binary_train_dtm = binary_vect.transform(X_train)\n",
    "binary_test_dtm = binary_vect.transform(X_test)\n",
    "# For the frequency model, training and test X matrices\n",
    "freq_train_dtm = freq_vect.transform(X_train)\n",
    "freq_test_dtm = freq_vect.transform(X_test)\n",
    "# For the tfidf model, training and test X matrices\n",
    "tfidf_train_dtm = tfidf_vect.transform(X_train)\n",
    "tfidf_test_dtm = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fit and transform operations can be performed in a single step by calling `fit_transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Performing Naive Bayes Classification\n",
    "`sklearn` has several different types of Naive Bayes Classifiers.\n",
    "\n",
    "####Binary Model\n",
    "For the binary model, the features are all assumed to be binary 0/1, which means the [`BernoulliNB`](http://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes) model of `sklearn` is appropriate.  Here's how we would train such a model and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_train_dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-035eab9bf1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Fit the model to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_train_dtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Score the model against the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_test_dtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'binary_train_dtm' is not defined"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Create the model\n",
    "bnb = BernoulliNB()\n",
    "# Fit the model to the training data\n",
    "bnb.fit(binary_train_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "bnb.score(binary_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Frequency Model\n",
    "For the frequency model, the [`MultinomialNB`](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) model is appropriate because it's assumed that each of the possible features is selected *k* times from a possible *n* (where *n* is the number of words in the document) and thus can be modeled by the [Multinomial Distribution](https://en.wikipedia.org/wiki/Multinomial_distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82666666666666666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Create the model\n",
    "mnb = MultinomialNB()\n",
    "# Fit the model to the training data\n",
    "mnb.fit(freq_train_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "mnb.score(freq_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####TFIDF Model\n",
    "Finally, let's try the same with our **TFIDF** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82333333333333336"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "tfidf_nb = MultinomialNB()\n",
    "# Fit the model to our training data\n",
    "tfidf_nb.fit(tfidf_train_dtm, y_train)\n",
    "# Score the model against our test data\n",
    "tfidf_nb.score(tfidf_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Classifier Model Evaluation\n",
    "As we've touched on, there a number of ways that one can evaluate the performance of a potential classification model.  Let's take a look at a few of them and how we can use them via the [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) package.  We'll use our basic frequency model for all of these.\n",
    "\n",
    "####Classification Accuracy\n",
    "The default `score()` method for a classifier returns the % correct accuracy of the classifier against a given test set.  It can be duplicated by calling the `metrics.accuracy_score()` function against a set of true labels and predicted labels.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82666666666666666"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import metrics\n",
    "from sklearn import metrics\n",
    "# Make class predictions for all observations in the test set\n",
    "y_pred = mnb.predict(freq_test_dtm)\n",
    "# Print classification accuracy\n",
    "metrics.accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Confusion Matrix\n",
    "The [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) returns a table that displays the 4 potential options, **true positive, true negative, false positive, false negative**, with counts for each in our test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126,  28],\n",
       "       [ 24, 122]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Precision, Recall, and F1\n",
    "We can also easily display the [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) and [F1](https://en.wikipedia.org/wiki/F1_score) scores for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.813333333333\n",
      "Recall: 0.835616438356\n",
      "F1: 0.824324324324\n",
      "Manual F1: 0.824324324324\n"
     ]
    }
   ],
   "source": [
    "# Retrieve precision\n",
    "p = metrics.precision_score(y_test, y_pred)\n",
    "# Retrieve Recall\n",
    "r = metrics.recall_score(y_test, y_pred)\n",
    "# Print precision and recall\n",
    "print 'Precision: ' + str(p)\n",
    "print 'Recall: ' + str(r)\n",
    "# Retrieve F1 from sklearn and print\n",
    "print 'F1: ' + str(metrics.f1_score(y_test, y_pred))\n",
    "# Calculate F1 manually and confirm they're the same\n",
    "print 'Manual F1: ' + str(2*p*r/(p + r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####ROC Curve\n",
    "As we've seen, we can generally increase our recall for a given class at the expense of precision and vice versa.  [ROC Curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) attempt to capture this relationship by ploting the **True Positive Rate** against the **False Positive Rate** across the spectrum of thresholds for predicting a positive class.  The extreme cases are predicting all classes to be positive (large false positive rate) or predicting none or very few to be positive (large true positive rate).  An ideal ROC curve would be a perfect right angle that maintains a 100% True Positive Rate even at 0 False Positive Rate, thus the closer a curve is to this ideal the better our model.  We can measure how close our ROC Curve is to ideal by measuring the area under the curve.\n",
    "\n",
    "We can easily retrieve ROC Curves with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve a series of False Positive Rate and True Positive Rate (with their respective thresholds) for plotting an ROC Curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, mnb.predict_proba(freq_test_dtm)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10bcb2910>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSISCQUREIhQMIiIS8f8BHCxwg3x2ozoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b9eec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot our ROC Curve\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(fpr, tpr, label='ROC Curve', color='red')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exercise - Spam Classification\n",
    "\n",
    "We'll step through a similar exercise with the spam classification dataset [here](https://github.com/pburkard88/DS_BOS_06/Data/Spam Classification/sms.csv) (note that this is a pre-parsed version of the one from the UCI website, so make sure to use this one).  The goal is to be able to classify raw text SMS messages as either spam or not spam based on the training set given.  We'll again use a Naive Bayes Multinomial model to do so.\n",
    "\n",
    "First import `numpy` and `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing numpy and pandas \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## READING IN THE DATA\n",
    "df = pd.read_csv(\"/Users/pburkard88/git/GA/Data Science/DAT_SF_14/labs/data/sms.csv\")\n",
    "# Check it out with head()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records are they of each type (spam/not spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ef5c9f690925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check out value_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check out value_counts\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want our *label* column to be binary, so use map to convert that column to 0 for 'ham' and 1 for 'spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                msg\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the label into a binary variable with map\n",
    "df['label'] = df.label.map({'ham': 0 , 'spam':1})\n",
    "# Use head() to make sure it worked\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a **70/30 train/test split** with [`sklearn.cross_validation.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) and put the results in `X_train`, `X_test`, `y_train`, and `y_test`.  Give the call to your function the parameter `random_state=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets by calling sklearn lib\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.msg, df.label, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some of `X_train` as well as the shape of `X_train` and `X_test` using `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "710     4mths half price Orange line rental & latest c...\n",
      "3740                           Did you stitch his trouser\n",
      "2711    Hope you enjoyed your new content. text stop t...\n",
      "3155    Not heard from U4 a while. Call 4 rude chat pr...\n",
      "3748    �� neva tell me how i noe... I'm not at home i...\n",
      "2389    wiskey Brandy Rum Gin Beer Vodka Scotch Shampa...\n",
      "3464    i am seeking a lady in the street and a freak ...\n",
      "772     Lol! U drunkard! Just doing my hair at d momen...\n",
      "3667    I'm turning off my phone. My moms telling ever...\n",
      "4955    U coming back 4 dinner rite? Dad ask me so i r...\n",
      "854     AH POOR BABY!HOPE URFEELING BETTERSN LUV! PROB...\n",
      "4079                  Gam gone after outstanding innings.\n",
      "2837                         Nice.nice.how is it working?\n",
      "1392                  Haha just kidding, papa needs drugs\n",
      "5533    Hey chief, can you give me a bell when you get...\n",
      "874     Ugh its been a long day. I'm exhausted. Just w...\n",
      "4408    Awesome, plan to get here any time after like ...\n",
      "3990    Ok lor. Anyway i thk we cant get tickets now c...\n",
      "1921                        Dont know you bring some food\n",
      "749     Is there a reason we've not spoken this year? ...\n",
      "2947                        make that 3! 4 fucks sake?! x\n",
      "2378    YES! The only place in town to meet exciting a...\n",
      "83                   You will be in the place of that man\n",
      "4668                           I send the print  outs da.\n",
      "128                                Are you there in room.\n",
      "4521    What to think no one saying clearly. Ok leave ...\n",
      "5090                             St andre, virgil's cream\n",
      "885     Yoyyooo u know how to change permissions for a...\n",
      "134     Sunshine Quiz Wkly Q! Win a top Sony DVD playe...\n",
      "3060    Dear all, as we know  &lt;#&gt; th is the  &lt...\n",
      "                              ...                        \n",
      "1031    Can not use foreign stamps in this country. Go...\n",
      "1110                      S s..first time..dhoni rocks...\n",
      "1888    Urgent! Please call 09061743811 from landline....\n",
      "3550    I got like $ &lt;#&gt; , I can get some more l...\n",
      "1527    Wow ... I love you sooo much, you know ? I can...\n",
      "753                           Dont gimme that lip caveboy\n",
      "3049             Die... Now i have e toot fringe again...\n",
      "2628    I know I'm lacking on most of this particular ...\n",
      "562                      Thanx 4 e brownie it's v nice...\n",
      "4764                           Prepare to be pleasured :)\n",
      "3562    Text BANNEDUK to 89555 to see! cost 150p texto...\n",
      "252     Wen ur lovable bcums angry wid u, dnt take it ...\n",
      "2516    Bognor it is! Should be splendid at this time ...\n",
      "2962    I'm doing da intro covers energy trends n pros...\n",
      "4453    I've told you everything will stop. Just dont ...\n",
      "5374    Do u konw waht is rael FRIENDSHIP Im gving yuo...\n",
      "5396             As in i want custom officer discount oh.\n",
      "1202                                 I know she called me\n",
      "3462    K.. I yan jiu liao... Sat we can go 4 bugis vi...\n",
      "2797    Tell your friends what you plan to do on Valen...\n",
      "4225    Double eviction this week - Spiral and Michael...\n",
      "144            I know you are. Can you pls open the back?\n",
      "5056    Am on a train back from northampton so i'm afr...\n",
      "2895                     K...k...yesterday i was in cbe .\n",
      "2763    ARR birthday today:) i wish him to get more os...\n",
      "905     We're all getting worried over here, derek and...\n",
      "5192    Oh oh... Den muz change plan liao... Go back h...\n",
      "3980    CERI U REBEL! SWEET DREAMZ ME LITTLE BUDDY!! C...\n",
      "235     Text & meet someone sexy today. U can find a d...\n",
      "5157                              K k:) sms chat with me.\n",
      "Name: msg, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of X_train\n",
    "print X_train.shape\n",
    "# Print X_train\n",
    "print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of X_test\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Generating Features\n",
    "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instantiate the vectorizer ( use variable name as vect)\n",
    "vect = CountVectorizer(decode_error = 'ignore')\n",
    "# Fit the vectorizer to the training set X_train\n",
    "vect.fit(X_train)\n",
    "# Print out the dictionary of terms with get_feature_names()\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a vectorizer trained on `X_train`, call `transform()` on both `X_train` and `X_test` to get the **Document Term Matrix** for each.  Store your results into `train_dtm` and `test_dtm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate train_dtm\n",
    "train_dtm = vect.transform(X_train)\n",
    "# Generate test_dtm\n",
    "test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 50 and last 50 features of your dataset by investigating `vect.get_feature_names()`.  ***HINT:*** It's just a `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'11', u'12', u'13', u'15', u'20', u'2005', u'2160', u'24', u'2mp', u'325', u'3o', u'42', u'44', u'4s', u'50', u'5020', u'510', u'5320', u'680', u'700w', u'8125', u'abhor', u'able', u'abound', u'about', u'above', u'absolutel', u'absolutely', u'ac', u'accept', u'acceptable', u'access', u'accessing', u'accidentally', u'accompanied', u'according', u'activate', u'activated', u'actually', u'ad', u'adapter', u'adapters', u'addition', u'additional', u'adhesive', u'adorable', u'advertised', u'advise', u'after']\n",
      "[u'white', u'who', u'whole', u'whose', u'wi', u'wife', u'wild', u'will', u'wind', u'window', u'winner', u'wiping', u'wire', u'wired', u'wireless', u'wise', u'wish', u'with', u'within', u'without', u'wobbly', u'won', u'wonder', u'wonderfully', u'wood', u'word', u'work', u'worked', u'working', u'works', u'world', u'worn', u'worst', u'worth', u'worthless', u'worthwhile', u'would', u'wouldn', u'wow', u'wrong', u'wrongly', u'year', u'years', u'yell', u'yes', u'yet', u'you', u'your', u'z500a', u'zero']\n"
     ]
    }
   ],
   "source": [
    "# First 50\n",
    "print vect.get_feature_names()[:50]\n",
    "# Last 50\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Build the Model\n",
    "Use `sklearn` to build a `MultinomialNB` classifier against your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Create model\n",
    "nb = MultinomialNB()\n",
    "# Fit model\n",
    "nb.fit(train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluate the Model\n",
    "Use `sklearn.metrics` to retrieve the following evaluation metrics for your model against the test set:\n",
    "- Percent Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- Confusion Matrix\n",
    "- ROC: Plot the curve and return the area under the curve\n",
    "- A classification report via a simple call to `classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test_dtm, call them y_pred\n",
    "y_pred = nb.predict(test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965541995693\n"
     ]
    }
   ],
   "source": [
    "# Percent Accuracy\n",
    "print metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895953757225\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "print metrics.precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837837837838\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "print metrics.recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865921787709\n"
     ]
    }
   ],
   "source": [
    "# F1\n",
    "print metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1190   18]\n",
      " [  30  155]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976293180598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10c481cd0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c3d8650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC Curve Area\n",
    "print metrics.roc_auc_score(y_test, nb.predict_proba(test_dtm)[:, 1])\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, nb.predict_proba(test_dtm)[:, 1])\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(fpr, tpr, color='red')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      1208\n",
      "          1       0.90      0.84      0.87       185\n",
      "\n",
      "avg / total       0.96      0.97      0.97      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print metrics.classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extra: Playing with pandas\n",
    "Use the arrays train_dtm and test_dtm along with y_train and y_test to see if you can put the data back into dataframes called `df_train` and `df_test`, where the columns are the feature names.\n",
    "- Get the feature names with `get_feature_names()`\n",
    "- Pass the `numpy` array data explicitly to the `pandas.DataFrame` constructor along with the column names from `get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>2005</th>\n",
       "      <th>2160</th>\n",
       "      <th>24</th>\n",
       "      <th>2mp</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yell</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>z500a</th>\n",
       "      <th>zero</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  11  12  13  15  20  2005  2160  24  2mp   ...    year  years  yell  \\\n",
       "0   0   0   0   0   0   0     0     0   0    0   ...       0      0     0   \n",
       "1   0   0   0   0   0   0     0     0   0    0   ...       0      0     0   \n",
       "2   0   0   0   0   0   0     0     0   0    0   ...       0      0     0   \n",
       "3   0   0   0   0   0   0     0     0   0    0   ...       0      0     0   \n",
       "4   0   0   0   0   0   0     0     0   0    0   ...       0      0     0   \n",
       "\n",
       "   yes  yet  you  your  z500a  zero  target  \n",
       "0    0    0    0     1      0     0       0  \n",
       "1    0    0    1     0      0     0       0  \n",
       "2    0    0    1     1      0     0       1  \n",
       "3    0    0    0     0      0     0     NaN  \n",
       "4    0    0    0     0      0     0     NaN  \n",
       "\n",
       "[5 rows x 1495 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "df_train['target']=y_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out which words are most indicative of spam vs. not spam, which words appear the most in the dataset, and which words the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
